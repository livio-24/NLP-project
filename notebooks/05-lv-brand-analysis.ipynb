{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac8568-f4b6-4a32-8482-621b561beb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b233a9e-f8c3-4f77-8d6d-08a58e120499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3e580-d0ed-4335-8110-2e77b271662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_review_sentiment_score(x):\n",
    "    tot = 0\n",
    "    card = 0\n",
    "    for word, _, score in x:\n",
    "        if score != 0:\n",
    "            tot += score\n",
    "            card += 1\n",
    "    \n",
    "    return tot/card if card != 0 else 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd7a9e-d6f1-4ad9-81be-70358e4108e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "dataset['tagged_text'] = dataset['tagged_text'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209ea0f-b00f-4be8-9057-b41e14c19c4b",
   "metadata": {},
   "source": [
    "I ratings sono da 1 a 5, mentre i sentiment scores da -5 a 5, quindi vengono normalizzati tra 1 e 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c20603-3f56-4f47-8523-f61bef7cfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_RSS(x):\n",
    "    norm = 0\n",
    "    if x >= 3:\n",
    "        norm = 5\n",
    "\n",
    "    elif 1 < x <= 3:\n",
    "        norm = 4\n",
    "\n",
    "    elif -0.5 < x <= 1:\n",
    "        norm = 3\n",
    "\n",
    "    elif -3 < x <= -0.5:\n",
    "        norm = 2\n",
    "\n",
    "    elif x <= -3 :\n",
    "        norm = 1\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebba91-e03f-43be-870c-521bb610c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'RSS' not in dataset.columns:\n",
    "    dataset['RSS'] = dataset['tagged_text'].apply(calculate_review_sentiment_score)\n",
    "    dataset['RSS'] = dataset['RSS'].apply(normalize_RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdce01-ca16-4205-8b95-ca2a8f364054",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b6c8f-79bd-4343-845c-0f253ea4bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['RSS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8d22f-4a5d-40a3-9764-1064cc048993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=dataset['RSS'].value_counts().index, values=dataset['RSS'].value_counts().values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1c73e-9953-4ca8-86a6-5758cd13a6d8",
   "metadata": {},
   "source": [
    "BERT per migliorare la fase di SA. Nel paper originale viene utilizzato il lexicon AFINN, che essendo appunto un aprroccio lexicon based ha molte limitazioni rispetto a BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0371bee-b9c8-40ef-966d-fe73e1561bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffad75-bee8-4a9d-8b4c-8c94bfc55ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94badf9c-282a-48e2-ab10-c9f107285221",
   "metadata": {},
   "source": [
    "link alla descrizione del modello utilizzato: https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment#bert-base-multilingual-uncased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b392ba-79da-41e9-9c04-9c8d34e50d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb51adc-f254-45db-8de7-22a91273c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ids_and_attention_mask_chunk(tokens):\n",
    "    \"\"\"\n",
    "    This function splits the input_ids and attention_mask into chunks of size 'chunksize'. \n",
    "    It also adds special tokens (101 for [CLS] and 102 for [SEP]) at the start and end of each chunk.\n",
    "    If the length of a chunk is less than 'chunksize', it pads the chunk with zeros at the end.\n",
    "    \n",
    "    Returns:\n",
    "        input_id_chunks (List[torch.Tensor]): List of chunked input_ids.\n",
    "        attention_mask_chunks (List[torch.Tensor]): List of chunked attention_masks.\n",
    "    \"\"\"\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "    \n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101], device=device), input_id_chunks[i], torch.tensor([102], device=device)\n",
    "        ])\n",
    "        \n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1], device=device), attention_mask_chunks[i], torch.tensor([1],device=device)\n",
    "        ])\n",
    "        \n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "        \n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.tensor([0] * pad_length, device=device)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.tensor([0] * pad_length, device=device)\n",
    "            ])\n",
    "            \n",
    "    return input_id_chunks, attention_mask_chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abfe44-7a15-4e93-b2a3-cbf991ff3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode_plus(review, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk(tokens)\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(attention_mask_chunks)\n",
    "    input_dict = {\n",
    "        'input_ids' : input_ids.long(),\n",
    "        'attention_mask' : attention_mask.int()\n",
    "    }\n",
    "    \n",
    "    outputs = model(**input_dict)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim = -1)\n",
    "    mean_probabilities = probabilities.mean(dim = 0)\n",
    "    \n",
    "    return torch.argmax(mean_probabilities).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58257ef-df33-4abd-a19b-9a2b509c564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ddcf1-86ac-480e-80b2-26171c9460b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if 'BERT_RSS' not in dataset.columns:\n",
    "    dataset['BERT_RSS'] = dataset['reviewText'].progress_apply(sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85680f01-b8c1-4b58-81b3-e6b946c4358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=dataset['BERT_RSS'].value_counts().index, values=dataset['BERT_RSS'].value_counts().values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc9a2e-4693-40bb-8fcc-c675cf37bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv('data/preprocessed_dataset.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8eb78-af6e-4b1d-91a9-f2540ffa59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta la larghezza massima per la visualizzazione delle colonne\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9bc10-b1b2-4602-b825-fe6ea668f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['BERT_RSS'] == 1) &( dataset['RSS'] >= 3)][['BERT_RSS','RSS','preprocessed_text']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b5a86-4ab1-47cb-a99b-7c0d5c0921cd",
   "metadata": {},
   "source": [
    "NLTK Punkt Sentence Tokenizer per splittare le reviews in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca47fb7-bea4-4662-9f46-eb71633cb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') #sentence_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468af86-4c23-497b-9260-75552f2e10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44c023-b0a4-4ae3-a34d-b185c6b30069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369af0a-16fa-4079-a803-87247a70a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5584d-6020-4921-b455-1ffa7423ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61fc9f-7ba5-4c9b-b60e-e853de0427d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per splittare le recensioni in frasi\n",
    "def preprocess_analyze_sentences(review, sa):\n",
    "    sentences_sentiments = {}\n",
    "    sentences = sent_tokenizer.tokenize(review)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        afinn_scores = []\n",
    "        sentence_clean = re.sub(r'[^\\w\\s]', '', sentence) #cleaning\n",
    "        # Tokenizza in parole\n",
    "        words = nltk.word_tokenize(sentence_clean)\n",
    "        # Rimuovi stopwords e applica lemmatization\n",
    "        preprocessed_words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in stop_words]\n",
    "        for word in preprocessed_words:\n",
    "            score = afinn.score(word)\n",
    "            if score != 0:\n",
    "                afinn_scores.append(score)\n",
    "        # Ricostruisci la frase preprocessata come stringa\n",
    "        preprocessed_sentence = ' '.join(preprocessed_words)\n",
    "        # Calcola il sentiment\n",
    "        if(sa == 'afinn'):\n",
    "            sentences_sentiments[preprocessed_sentence] = normalize_RSS((sum(afinn_scores)/len(afinn_scores))) if len(afinn_scores) != 0 else normalize_RSS(0)\n",
    "        elif(sa == 'bert'):\n",
    "            sentences_sentiments[preprocessed_sentence] = sentiment_score(sentence)\n",
    "        \n",
    "        #sentences_sentiments.append((preprocessed_sentence, (sum(afinn_scores)/len(afinn_scores)) if len(afinn_scores) != 0 else 0))\n",
    "\n",
    "    return sentences_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8d033-ebb6-4801-86a5-38cf7145399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per combinare tutti i dizionari di sentences per ogni 'asin'\n",
    "def combine_sentences(sent_dicts):\n",
    "    combined_dict = {}\n",
    "    for d in sent_dicts:\n",
    "        combined_dict.update(d)\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab32eb2-1d3b-49ef-8f35-5379f8982f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = \"\"\"Motorola Q Sucks. If you are a serious user of cell phone, then don't buy this.\n",
    "The phone is not good. That is the reason, Verizon has brought down the price from \"\"\"\n",
    "\n",
    "preprocess_analyze_sentences(r, sa='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2026a-75c6-41bc-bfe5-02ab048612ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungere il campo features basato sulla lista di features\n",
    "def extract_features(sent_dict, features_list):\n",
    "    feature_sentiments = {}\n",
    "    for feature in features_list:\n",
    "        feature_values = [sentiment for sentence, sentiment in sent_dict.items() if feature in sentence.split()]\n",
    "        if feature_values:\n",
    "            feature_sentiments[feature] = sum(feature_values) / len(feature_values)\n",
    "    return feature_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440e282-99ce-459f-b2ff-c76a5b32354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "filtered_features = pd.read_csv('data/ontology_2_filtered.csv', sep=\";\")\n",
    "# Applicare la funzione a ogni recensione e raggruppare per 'asin'\n",
    "dataset['preprocessed_sentences'] = dataset['reviewText'].progress_apply(preprocess_analyze_sentences, args=('bert',))\n",
    "\n",
    "# Raggruppare per 'asin' e combinare le sentences\n",
    "grouped = dataset.groupby('asin')['preprocessed_sentences'].progress_apply(list).reset_index()\n",
    "grouped['preprocessed_sentences'] = grouped['preprocessed_sentences'].progress_apply(combine_sentences)\n",
    "grouped['features'] = grouped['preprocessed_sentences'].progress_apply(lambda x: extract_features(x, filtered_features['value']))\n",
    "\n",
    "result = grouped.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7b1af-5712-49ce-ba0d-bf560a16cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salva il risultato in un file JSON\n",
    "with open('data/bert_features_scores2.json', 'w') as f:\n",
    "      json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd9ff7-77a6-48fc-b25d-67910fc3f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a84423-3600-4cbf-83ad-1758cc6bcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_project.config import PROCESSED_DATA_DIR, INTERIM_DATA_DIR, FIGURES_DIR\n",
    "from nlp_project.sentiment_analysis import json_sentences_and_features_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0bab1-935e-483a-ac37-e63e30896e50",
   "metadata": {},
   "source": [
    "# BRAND ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d2677-c7dd-499f-b5ce-2f0c039c3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(PROCESSED_DATA_DIR / 'preprocessed_dataset.csv', sep=';')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ae83c-cde0-4d41-adf9-465a7c4b8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_top5_brands = dataset[dataset['brand'].isin(['Samsung', 'Motorola', 'Apple', 'BlackBerry', 'LG'])]\n",
    "dataset_top5_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a672791-fc7c-4d7a-963a-a77aabe43eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Raggruppiamo le recensioni per anno e contiamo quante ce ne sono per ogni anno\n",
    "reviews_per_year = dataset_top5_brands.groupby('year').size().reset_index(name='count')\n",
    "# Creiamo il grafico a barre con Plotly\n",
    "fig = px.bar(reviews_per_year, x='year', y='count', title='Numero di Recensioni per Anno', \n",
    "             labels={'year':'Anno', 'count':'Numero di Recensioni'}, \n",
    "             text='count')\n",
    "\n",
    "# Mostriamo il grafico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55aca7-dc84-458e-8cd0-ff380f49b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_top5_brands[dataset_top5_brands['year'] >= 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6edfd8-beb5-4918-b136-c25ae702e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per contare le occorrenze delle features in una review\n",
    "def count_features_in_review(review, features):\n",
    "    feature_count = {}\n",
    "    for feature in features:\n",
    "        # Trova il numero di occorrenze della feature nel testo della review (case insensitive)\n",
    "        feature_count[feature] = len(re.findall(r'\\b' + re.escape(feature) + r'\\b', review.lower()))\n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d483437-5464-4aa4-be35-49675c03f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(PROCESSED_DATA_DIR / 'ontology_filtered.csv', sep=';').iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb782e-73bf-483d-9d2a-7eb90d881c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la funzione alle review per contare le features\n",
    "df['feature_counts'] = df['reviewText'].apply(lambda x: count_features_in_review(x, features))\n",
    "\n",
    "# Trasforma il dizionario di conteggi in colonne individuali per ogni feature\n",
    "feature_df = pd.DataFrame(df['feature_counts'].tolist(), index=df.index)\n",
    "\n",
    "# Aggiungi le colonne delle features al dataframe originale\n",
    "df = pd.concat([df, feature_df], axis=1)\n",
    "\n",
    "#Raggeùruppa per brand e sommare le occorrenze delle features\n",
    "brand_feature_freq = df.groupby('brand')[features].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18c543-2339-4cf2-9089-14b23a511358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brand_feature_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df54a4-91a5-41a7-83f7-1052365b1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_features = {}\n",
    "k = 5\n",
    "# Itera su ciascun brand e crea un plot con le 5 features più frequenti\n",
    "for brand in brand_feature_freq.index:\n",
    "    # Ordina le features per frequenza e prendi le 5 più frequenti\n",
    "    top_10_features = brand_feature_freq.loc[brand].nlargest(k)\n",
    "    top_features[brand] = top_10_features.index.to_list()\n",
    "    # Creazione del grafico\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=top_10_features.values, y=top_10_features.index, palette=\"Blues_d\")\n",
    "    \n",
    "    # Aggiungi titolo e label\n",
    "    plt.title(f'Top {k} Features for {brand}', fontsize=16)\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    \n",
    "    # Mostra il grafico\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / f'{brand}_most_cited_features.png', format='png')  # Salva con il nome del brand\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e7199-fa13-4d8f-b10c-d8dbba27bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4d707-0372-4507-9b8a-54ca0fe1555e",
   "metadata": {},
   "source": [
    "## come è cambiato nel tempo il sentiment nei confronti di queste features, per ciascun brand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0d1df-9ee6-42e0-9945-1dae67c0a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dizionario per salvare i DataFrame per ciascun brand e anno\n",
    "brand_year_dfs = {}\n",
    "\n",
    "for brand in df['brand'].unique():\n",
    "    for year in df['year'].unique():\n",
    "        # Filtra il dataframe per il brand e l'anno corrente\n",
    "        filtered_df = df[(df['brand'] == brand) & (df['year'] == year)]\n",
    "        \n",
    "        # Salva il dataframe in un dizionario con chiave come (brand, year)\n",
    "        brand_year_dfs[(brand, year)] = filtered_df\n",
    "\n",
    "        # Opzionale: puoi stampare la dimensione del dataframe o una preview\n",
    "        #print(f\"DataFrame per {brand} nel {year}:\")\n",
    "        #print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0f281-0d38-4479-805e-6e011816e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brand_year_dfs[('Apple', 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acbee8-aa24-407d-a4ba-c246e69b047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') #sentence_tokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f1e39-92c0-42c4-92e0-0bed1ae82aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "def calculate_features_scores_brand_year(brand_year_dfs, filtered_features): \n",
    "    for (brand, year), df in brand_year_dfs.items(): \n",
    "        print(f\"Calcolo delle features per {brand} nel {year}\")\n",
    "        json_sentences_and_features_scores(df,filtered_feautes, lemmatizer, stop_words, afinn, sent_tokenizer, save_path=f'brand_analysis/bert_features_scores_{brand}_{year}.json')\n",
    "        # Applicare la funzione a ogni recensione e raggruppare per 'asin'\n",
    "        #df['preprocessed_sentences'] = df['reviewText'].progress_apply(preprocess_analyze_sentences, args=('bert',))\n",
    "        \n",
    "        # Raggruppare per 'asin' e combinare le sentences\n",
    "        #grouped = df.groupby('asin')['preprocessed_sentences'].progress_apply(list).reset_index()\n",
    "        #grouped['preprocessed_sentences'] = grouped['preprocessed_sentences'].progress_apply(combine_sentences)\n",
    "        #grouped['features'] = grouped['preprocessed_sentences'].progress_apply(lambda x: extract_features(x, filtered_features['value']))\n",
    "        \n",
    "        #result = grouped.to_dict(orient='records')\n",
    "        #Salva il risultato in un file JSON\n",
    "        #with open(f'data/bert_features_scores_{brand}_{year}.json', 'w') as f:\n",
    "        #  json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e379a-f4fd-4654-9c84-293c3b38eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_features_scores_brand_year(brand_year_dfs, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef22be-4170-4edf-b25f-e6769440181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Definisci la directory che contiene i file JSON\n",
    "json_directory = PROCESSED_DATA_DIR / 'brands_analysis/'\n",
    "\n",
    "# Dizionario per salvare le medie delle features per ciascun brand e anno\n",
    "brand_year_averages = {}\n",
    "\n",
    "# Itera su ciascun file JSON nella directory\n",
    "for filename in os.listdir(json_directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Estrai brand e anno dal nome del file (es: Apple_2018.json)\n",
    "        brand, year = filename.replace(\".json\", \"\").split('_')\n",
    "        \n",
    "        # Carica il file JSON\n",
    "        with open(os.path.join(json_directory, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Dizionario per accumulare i punteggi delle features\n",
    "        feature_scores = defaultdict(list)\n",
    "        \n",
    "        # Itera su ciascun prodotto nel file JSON\n",
    "        for product in data:\n",
    "            # Prendi il dizionario delle features e i relativi punteggi\n",
    "            features = product.get('features', {})\n",
    "            \n",
    "            # Accumula i punteggi per ciascuna feature\n",
    "            for feature, score in features.items():\n",
    "                feature_scores[feature].append(score)\n",
    "        \n",
    "        # Ora possiamo calcolare la media per ciascuna feature\n",
    "        feature_means = {feature: sum(scores) / len(scores) for feature, scores in feature_scores.items()}\n",
    "        \n",
    "        # Salva le medie nel dizionario brand_year_averages\n",
    "        brand_year_averages[(brand, year)] = feature_means\n",
    "\n",
    "        # Opzionale: stampa le medie per il brand e anno corrente\n",
    "        #print(f\"Medie delle features per {brand} nel {year}:\")\n",
    "        #print(feature_means)\n",
    "\n",
    "# Converti il dizionario brand_year_averages in un DataFrame per una visualizzazione più comoda\n",
    "df_averages = pd.DataFrame.from_dict(brand_year_averages, orient='index')\n",
    "\n",
    "# Mostra il DataFrame finale\n",
    "#print(df_averages)\n",
    "\n",
    "# Opzionale: salva il DataFrame su disco in formato CSV\n",
    "#df_averages.to_csv('feature_averages_per_brand_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f225bd4-fe0d-458e-97c2-8c20fed58df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7b838-f6f1-487e-9825-db6df797ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9612389-0268-41f5-aeee-e2ae93feafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_averages = df_averages.reset_index(drop=False)\n",
    "\n",
    "# Lista dei brand disponibili nel DataFrame\n",
    "brands = df_averages['level_0'].unique()  # level_0 contiene i brand\n",
    "\n",
    "# Itera su ciascun brand\n",
    "for brand in brands:\n",
    "    # Filtra il DataFrame per il brand corrente\n",
    "    df_brand = df_averages[df_averages['level_0'] == brand]\n",
    "    \n",
    "    # Filtra il DataFrame per includere solo le colonne battery e screen\n",
    "    df_brand_filtered = df_brand[['level_1'] + ['battery', 'screen']]  # 'level_1' contiene l'anno\n",
    "\n",
    "    # Rinominare le colonne se necessario\n",
    "    df_brand_filtered.rename(columns={'level_1': 'year'}, inplace=True)\n",
    "\n",
    "    # Converti la colonna 'year' in formato numerico\n",
    "    df_brand_filtered['year'] = df_brand_filtered['year'].astype(int)\n",
    "\n",
    "    # Raggruppa per anno per ottenere la media delle features per ogni anno (se ci fossero dati duplicati)\n",
    "    df_grouped_brand = df_brand_filtered.groupby('year').mean()\n",
    "    df_cumulative = df_grouped_brand.expanding().mean()\n",
    "\n",
    "    # Crea il grafico di linee per ogni feature per il brand corrente\n",
    "    plt.figure(figsize=(6,4))\n",
    "\n",
    "    for feature in ['battery', 'screen']:\n",
    "        plt.plot(df_cumulative.index, df_cumulative[feature], marker='o', label=feature)\n",
    "\n",
    "    # Aggiungi titolo e etichette\n",
    "    plt.title(f\"trend screen e battery per {brand} (2013-2018)\")\n",
    "    plt.xlabel(\"Anno\")\n",
    "    plt.ylabel(\"Media cumulata dei Punteggi delle Features\")\n",
    "    plt.legend(title=\"Features\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / f'{brand}_features_trend.png', format='png')  # Salva con il nome del brand\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0aa12a-b487-4afa-89a6-151a01f371c2",
   "metadata": {},
   "source": [
    "1. Le feature 'screen'e 'battery' per il brand BlackBerry hanno mostrato un graduale miglioramento nella percezione dei consumatori, come indicato dall'aumento costante del sentiment cumulativo dal 2013 al 2018. Questo riflette probabilmente i miglioramenti tecnologici e l'importanza crescente dello schermo e della batteria negli smartphone BlackBerry.\n",
    "\n",
    "2. mentre per gli altri brand si nota un trend decrescente nella percezione dei consumatori per queste due features, il che può indicare il fatto che siano necessari dei miglioramenti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
